{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Theoretical Questions**\n",
        "\n",
        "### **1. What is unsupervised learning in the context of machine learning?**\n",
        "\n",
        "  - Unsupervised learning involves training models on data **without labeled outputs**. The goal is to discover **patterns**, **groupings**, or **structure** in the data (e.g., clustering, dimensionality reduction).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How does the K-Means clustering algorithm work?**\n",
        "\n",
        "  - K-Means works as follows:\n",
        "\n",
        "    1. Choose `k` initial cluster centroids randomly.\n",
        "    2. Assign each data point to the nearest centroid.\n",
        "    3. Update centroids as the mean of all assigned points.\n",
        "    4. Repeat steps 2 and 3 until convergence (no significant change in centroids).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Explain the concept of a dendrogram in hierarchical clustering.**\n",
        "\n",
        "  - A dendrogram is a **tree-like diagram** that shows the **merging or splitting** of clusters in **hierarchical clustering**. The height of each merge represents the **distance** or **dissimilarity** between clusters.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What is the main difference between K-Means and Hierarchical Clustering?**\n",
        "\n",
        "  * **K-Means**: Requires predefining `k`; uses centroid-based partitioning.\n",
        "  * **Hierarchical**: Builds a tree (dendrogram); does not require predefined `k`.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What are the advantages of DBSCAN over K-Means?**\n",
        "\n",
        "  * Can find **arbitrarily shaped** clusters.\n",
        "  * Doesn’t require specifying number of clusters.\n",
        "  * Can **identify outliers** (noise).\n",
        "\n",
        "---\n",
        "\n",
        "### **6. When would you use Silhouette Score in clustering?**\n",
        "\n",
        "  - To **evaluate clustering quality** by measuring how similar a point is to its own cluster vs. other clusters. Used for choosing the **optimal number of clusters**.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. What are the limitations of Hierarchical Clustering?**\n",
        "\n",
        "  * **Computationally expensive** for large datasets.\n",
        "  * **Irreversible**: once merged/split, can't be undone.\n",
        "  * Sensitive to **noise** and **outliers**.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Why is feature scaling important in clustering algorithms like K-Means?**\n",
        "\n",
        "  - Because K-Means uses **distance metrics** (like Euclidean), and features with large scales can dominate clustering results. Scaling ensures **fair influence**.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. How does DBSCAN identify noise points?**\n",
        "\n",
        "  - DBSCAN labels a point as **noise** if it's **not a core point** and is **not within the neighborhood** of any core point (based on `eps` and `min_samples`).\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Define inertia in the context of K-Means.**\n",
        "\n",
        "  - Inertia is the **sum of squared distances** of samples to their closest cluster center. Lower inertia indicates more compact clusters.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What is the elbow method in K-Means clustering?**\n",
        "\n",
        "  - A technique to determine optimal `k` by plotting **inertia vs. k**. The “elbow” point is where inertia reduction slows, indicating a good `k`.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. Describe the concept of \"density\" in DBSCAN.**\n",
        "\n",
        "  - In DBSCAN, density refers to the **number of points** within a given radius (`eps`). High density areas form clusters; low density areas are noise.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. Can hierarchical clustering be used on categorical data?**\n",
        "\n",
        "  - Yes, but only with **appropriate distance measures** (e.g., Hamming distance). Standard linkage methods assume numerical data.\n",
        "\n",
        "---\n",
        "\n",
        "### **14. What does a negative Silhouette Score indicate?**\n",
        "\n",
        "  - It indicates that the point is **likely misclassified**, being **closer to a different cluster** than its own.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. Explain the term \"linkage criteria\" in hierarchical clustering.**\n",
        "\n",
        "  - Linkage criteria determine how distances between clusters are calculated. Common types: **single**, **complete**, **average**, and **ward**.\n",
        "\n",
        "---\n",
        "\n",
        "### **16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?**\n",
        "\n",
        "  - Because it assumes **equal-sized, spherical clusters**. Varying sizes or densities violate these assumptions, causing misclassification.\n",
        "\n",
        "---\n",
        "\n",
        "### **17. What are the core parameters in DBSCAN, and how do they influence clustering?**\n",
        "\n",
        "  * **`eps`**: radius around a point.\n",
        "  * **`min_samples`**: minimum points in `eps` to form a core point.\n",
        "  These determine **cluster formation** and **noise identification**.\n",
        "\n",
        "---\n",
        "\n",
        "### **18. How does K-Means++ improve upon standard K-Means initialization?**\n",
        "\n",
        "  - K-Means++ selects **initial centroids more strategically** to be far apart, leading to **faster convergence** and **better clustering**.\n",
        "\n",
        "---\n",
        "\n",
        "### **19. What is agglomerative clustering?**\n",
        "\n",
        "  - A bottom-up hierarchical clustering method where **each point starts as its own cluster**, and clusters are **merged iteratively**.\n",
        "\n",
        "---\n",
        "\n",
        "### **20. What makes Silhouette Score a better metric than just inertia for model evaluation?**\n",
        "\n",
        "  - Silhouette Score considers **both cohesion and separation**, while inertia only considers **within-cluster distance**. Hence, it's a **more holistic** measure.\n"
      ],
      "metadata": {
        "id": "ZR6oR4agSLw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practical Questions**\n"
      ],
      "metadata": {
        "id": "CYeaMUzkVhJa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FeO4wIGSFIj"
      },
      "outputs": [],
      "source": [
        "#21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Generate synthetic data with 4 centers\n",
        "X, y_true = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=42)\n",
        "\n",
        "# Step 2: Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# Step 3: Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, alpha=0.75, marker='X', label='Centroids')\n",
        "plt.title('K-Means Clustering on Synthetic Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22.  Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "\n",
        "# Step 2: Apply Agglomerative Clustering with 3 clusters\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Step 3: Display the first 10 predicted labels\n",
        "print(\"First 10 predicted labels:\", labels[:10])\n"
      ],
      "metadata": {
        "id": "IJ64_d_gVg1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plo\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Step 1: Generate synthetic moon-shaped data\n",
        "X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
        "\n",
        "# Step 2: Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot results\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Assign color: -1 is noise (outlier), others are cluster labels\n",
        "unique_labels = set(labels)\n",
        "colors = ['red' if label == -1 else plt.cm.Set1(label / max(unique_labels)) for label in labels]\n",
        "\n",
        "# Plot each point with its color\n",
        "for i in range(len(X)):\n",
        "    plt.scatter(X[i, 0], X[i, 1], color=colors[i], edgecolor='k', s=50)\n",
        "\n",
        "plt.title(\"DBSCAN Clustering with Outliers Highlighted\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "UNoBshNfWC0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Step 2: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Step 4: Print the size of each cluster\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "cluster_sizes = dict(zip(unique, counts))\n",
        "\n",
        "print(\"Cluster sizes:\")\n",
        "for cluster_id, size in cluster_sizes.items():\n",
        "    print(f\"Cluster {cluster_id}: {size} samples\")\n"
      ],
      "metadata": {
        "id": "WZdwL-b-WRDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Step 1: Generate circular synthetic data\n",
        "X, _ = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=0)\n",
        "\n",
        "# Step 2: Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot the result\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Assign unique colors to each cluster, and black to noise\n",
        "unique_labels = set(labels)\n",
        "for label in unique_labels:\n",
        "    label_mask = (labels == label)\n",
        "    color = 'k' if label == -1 else plt.cm.Set1(label / max(unique_labels))\n",
        "    plt.scatter(X[label_mask, 0], X[label_mask, 1],\n",
        "                label=f'Cluster {label}' if label != -1 else 'Noise',\n",
        "                s=50, edgecolor='k', c=[color])\n",
        "\n",
        "plt.title(\"DBSCAN Clustering on make_circles Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dt034pY5WcRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "\n",
        "# Step 2: Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply K-Means with 2 clusters\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Step 4: Output cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "centroids_df = pd.DataFrame(centroids, columns=data.feature_names)\n",
        "\n",
        "print(\"Cluster Centroids (scaled features):\")\n",
        "print(centroids_df)\n"
      ],
      "metadata": {
        "id": "t5ncj7GMWmjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=3, cluster_std=[0.2, 1.0, 2.5], random_state=42)\n",
        "\n",
        "db = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k')\n",
        "plt.title(\"DBSCAN with Varying Cluster Standard Deviations\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eiUqSHn7WyHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=50)\n",
        "plt.title(\"K-Means Clusters on Digits Dataset (PCA)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u3Q_Euk9YVZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=0.6, random_state=0)\n",
        "\n",
        "scores = []\n",
        "ks = range(2, 6)\n",
        "for k in ks:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    scores.append(silhouette_score(X, labels))\n",
        "\n",
        "plt.bar(ks, scores, color='skyblue')\n",
        "plt.title(\"Silhouette Scores for K = 2 to 5\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sP3DjRtEYYnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "linkage_matrix = linkage(X, method='average')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linkage_matrix, labels=iris.target)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram (Average Linkage)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0LsptwypYYjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.5, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.predict(X)\n",
        "\n",
        "# Plot decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='red', marker='X')\n",
        "plt.title(\"K-Means with Decision Boundaries\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oL4ydoZPYYhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
        "db = DBSCAN(eps=3, min_samples=5)\n",
        "labels = db.fit_predict(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=40)\n",
        "plt.title(\"DBSCAN on Digits (t-SNE Reduced)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0LZIOelsYYeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', edgecolor='k')\n",
        "plt.title(\"Agglomerative Clustering with Complete Linkage\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pJQwpC89YYcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = load_breast_cancer().data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "inertias = []\n",
        "ks = range(2, 7)\n",
        "for k in ks:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(ks, inertias, marker='o')\n",
        "plt.title(\"Inertia for K = 2 to 6 (Breast Cancer Dataset)\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XcWx15kyYYZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=400, noise=0.05, factor=0.5, random_state=0)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm', edgecolor='k')\n",
        "plt.title(\"Agglomerative Clustering with Single Linkage on Circles\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S2zJMxcQYYXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = load_wine().data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "db = DBSCAN(eps=1.5, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)\n"
      ],
      "metadata": {
        "id": "XjAvd1qsYYUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centroids')\n",
        "plt.legend()\n",
        "plt.title(\"KMeans Clustering with Cluster Centers\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "867L_uqyYYSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = load_iris().data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "db = DBSCAN(eps=0.6, min_samples=5)\n",
        "labels = db.fit_predict(X_scaled)\n",
        "\n",
        "n_noise = list(labels).count(-1)\n",
        "print(\"Number of noise samples:\", n_noise)\n"
      ],
      "metadata": {
        "id": "Kem5vSfeYYPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_moons(n_samples=300, noise=0.1, random_state=0)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Accent', edgecolor='k')\n",
        "plt.title(\"K-Means on make_moons (Non-linear Data)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EJUt3-FnYYMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "X = load_digits().data\n",
        "pca = PCA(n_components=3)\n",
        "X_3d = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_3d)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "sc = ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=labels, cmap='tab10', s=40)\n",
        "plt.title(\"3D PCA + KMeans on Digits Dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jgj0x-KfYYJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.6, random_state=42)\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "score = silhouette_score(X, labels)\n",
        "print(\"Silhouette Score for 5-cluster KMeans:\", score)\n"
      ],
      "metadata": {
        "id": "kel6pY7TYYAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = load_breast_cancer().data\n",
        "X_pca = PCA(n_components=2).fit_transform(X)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='coolwarm', edgecolor='k')\n",
        "plt.title(\"Agglomerative Clustering on Breast Cancer (PCA Reduced)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CwFzqMzyYXuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side.\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=500, noise=0.05, factor=0.5, random_state=0)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42).fit(X)\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"KMeans Clustering\")\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='Accent', edgecolor='k')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap='Accent', edgecolor='k')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GSqzrTnJZm7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = load_iris().data\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "labels = kmeans.labels_\n",
        "sil_samples = silhouette_samples(X, labels)\n",
        "\n",
        "plt.bar(range(len(sil_samples)), sil_samples, color='skyblue')\n",
        "plt.title(\"Silhouette Coefficient for Each Sample (Iris Dataset)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Silhouette Coefficient\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IFR_dGAOZmwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=400, centers=4, random_state=42)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Set1', edgecolor='k')\n",
        "plt.title(\"Agglomerative Clustering (Average Linkage)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gWLSXKNIZmmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features).\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "data = load_wine()\n",
        "X = pd.DataFrame(data.data[:, :4], columns=data.feature_names[:4])\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "X['Cluster'] = kmeans.fit_predict(data.data)\n",
        "\n",
        "sns.pairplot(X, hue='Cluster', palette='Set1', corner=True)\n",
        "plt.suptitle(\"KMeans Clustering (First 4 Features of Wine Dataset)\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6ulsLiR_ZmbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X, _ = make_blobs(n_samples=400, centers=3, cluster_std=1.2, random_state=42)\n",
        "\n",
        "db = DBSCAN(eps=0.9, min_samples=5)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(\"Number of clusters found:\", n_clusters)\n",
        "print(\"Number of noise points:\", n_noise)\n"
      ],
      "metadata": {
        "id": "e_jXXgwSZmS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = load_digits().data\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=40, edgecolor='k')\n",
        "plt.title(\"Agglomerative Clustering on Digits (t-SNE Reduced)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mEcKKa26ZmIT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}